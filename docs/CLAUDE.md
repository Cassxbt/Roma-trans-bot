# ROMA Translation Bot - Free & Open Source Edition

## Project Overview

An intelligent, multi-language translation system built on the ROMA (Recursive-Open-Meta-Agent) framework using **100% FREE and open-source resources**. Perfect for independent developers with zero cloud costs.

---

## üéØ Core Mission

Build a production-ready translation bot that:
1. Uses only free and open-source tools (no paid APIs)
2. Runs entirely locally or on free hosting tiers
3. Translates text across 12+ languages with quality
4. Handles complex multi-language requests through ROMA's intelligent task decomposition
5. Processes translations in parallel for optimal speed
6. Provides multiple interfaces (CLI, API, Web)

---

## üí∞ ZERO-COST ARCHITECTURE

### Free Resources Used

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  INTERFACE LAYER (FREE)                  ‚îÇ
‚îÇ  CLI | REST API | Web UI | Discord Bot (Free tier)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ROMA CORE ENGINE (FREE)                     ‚îÇ
‚îÇ  Open-source framework from Sentient AGI                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           FREE LLM OPTIONS (Choose One)                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Option 1: Ollama (LOCAL - Best for privacy)            ‚îÇ
‚îÇ    ‚Ä¢ Runs on your machine                               ‚îÇ
‚îÇ    ‚Ä¢ Models: Llama 3.1, Mistral, Qwen                   ‚îÇ
‚îÇ    ‚Ä¢ No API costs ever                                  ‚îÇ
‚îÇ    ‚Ä¢ Requires: 8GB+ RAM, decent CPU/GPU                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Option 2: Hugging Face Inference API (FREE)            ‚îÇ
‚îÇ    ‚Ä¢ Free tier: 1000 requests/day                       ‚îÇ
‚îÇ    ‚Ä¢ Models: Llama, Mistral, Qwen                       ‚îÇ
‚îÇ    ‚Ä¢ No credit card required                            ‚îÇ
‚îÇ    ‚Ä¢ Online inference                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Option 3: Together AI (FREE tier)                      ‚îÇ
‚îÇ    ‚Ä¢ $25 free credits (never expires for testing)       ‚îÇ
‚îÇ    ‚Ä¢ Fast inference                                     ‚îÇ
‚îÇ    ‚Ä¢ Multiple open models                               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Option 4: Groq (FREE tier)                             ‚îÇ
‚îÇ    ‚Ä¢ Fast inference (fastest free option!)              ‚îÇ
‚îÇ    ‚Ä¢ Limited free requests                              ‚îÇ
‚îÇ    ‚Ä¢ Great for demos                                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         FREE STORAGE & SERVICES                          ‚îÇ
‚îÇ  ‚Ä¢ SQLite (local database - free forever)               ‚îÇ
‚îÇ  ‚Ä¢ Redis Stack (free local cache)                       ‚îÇ
‚îÇ  ‚Ä¢ Local file storage (no S3 needed)                    ‚îÇ
‚îÇ  ‚Ä¢ Render.com (free web hosting)                        ‚îÇ
‚îÇ  ‚Ä¢ Railway.app (free tier with GitHub)                  ‚îÇ
‚îÇ  ‚Ä¢ Fly.io (free tier for small apps)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Technology Stack (100% Free)

### Backend
```yaml
Core Framework:
  - ROMA Framework (FREE - MIT License)
  - Python 3.12+ (FREE)
  - AgnoAgents (FREE)

LLM Options (Choose Based on Your Needs):
  Primary: Ollama (LOCAL - Completely FREE)
    - llama3.1:8b (4.7GB) - Best balance
    - mistral:7b (4.1GB) - Fast & efficient  
    - qwen2.5:7b (4.7GB) - Excellent for multilingual
    
  Backup: Hugging Face Inference API (FREE tier)
    - meta-llama/Meta-Llama-3.1-8B-Instruct
    - mistralai/Mistral-7B-Instruct-v0.3
    - 1000 requests/day free
    
  Alternative: Groq (FREE tier - Fastest!)
    - llama-3.1-8b-instant
    - Very fast inference
    - Great for demos

API Layer:
  - FastAPI (FREE)
  - Uvicorn (FREE)

Database:
  - SQLite (FREE - built-in Python)
  - No PostgreSQL needed!
  
Cache:
  - Redis Stack (FREE local)
  - OR Python dict cache (in-memory, completely free)

File Storage:
  - Local filesystem (FREE)
  - NO S3 required!

NLP Tools:
  - langdetect (FREE)
  - lingua-language-detector (FREE - more accurate)
  
File Processing:
  - PyPDF2 (FREE)
  - python-docx (FREE)
  - markdown (FREE)
```

### Free Hosting Options

```yaml
Web Hosting (Choose One):
  
  Option 1: Render.com (Recommended)
    - Free tier: 750 hours/month
    - Auto-deploy from GitHub
    - Free SSL certificate
    - Perfect for our API
    
  Option 2: Railway.app
    - $5 free credits monthly
    - GitHub integration
    - Easy deployment
    
  Option 3: Fly.io
    - Free tier: 3 shared VMs
    - Global deployment
    - Quick scaling
    
  Option 4: PythonAnywhere (For simple APIs)
    - Free tier available
    - Python-focused
    - Easy setup

Local Development:
  - Docker Desktop (FREE)
  - VS Code (FREE)
  - Git (FREE)
```

---

## üìÅ Project Structure (Simplified for Free Tier)

```
roma-translation-bot-free/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CLAUDE.md
‚îú‚îÄ‚îÄ LICENSE (MIT)
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile (optional)
‚îú‚îÄ‚îÄ render.yaml           # Render.com deployment
‚îú‚îÄ‚îÄ railway.toml          # Railway.app deployment
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ agent_config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ languages.yaml
‚îÇ   ‚îî‚îÄ‚îÄ models.yaml       # FREE model configurations
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ translation_agent.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ roma_wrapper.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config_loader.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ executors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ language_detection.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ translation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quality_check.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ format_preservation.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py      # FREE LLM integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache_service.py    # Simple in-memory cache
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database_service.py # SQLite operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_service.py     # Local file handling
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ translation.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ request.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ response.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commands.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ logger.py
‚îÇ       ‚îî‚îÄ‚îÄ validators.py
‚îÇ
‚îú‚îÄ‚îÄ data/                # Local data storage
‚îÇ   ‚îú‚îÄ‚îÄ translations.db  # SQLite database
‚îÇ   ‚îú‚îÄ‚îÄ cache/          # File cache
‚îÇ   ‚îî‚îÄ‚îÄ uploads/        # Uploaded files
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îî‚îÄ‚îÄ integration/
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ setup_ollama.sh   # Setup Ollama locally
‚îÇ   ‚îú‚îÄ‚îÄ download_models.sh # Download free models
‚îÇ   ‚îî‚îÄ‚îÄ setup_db.py
‚îÇ
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ FREE_SETUP.md     # Guide for free resources
    ‚îî‚îÄ‚îÄ DEPLOYMENT.md     # Free deployment guide
```

---

## üöÄ Features (Free Edition)

### Phase 1: MVP (Free Resources)
- ‚úÖ Translation using FREE local models (Ollama)
- ‚úÖ Support for 12+ languages
- ‚úÖ CLI interface
- ‚úÖ REST API with FastAPI
- ‚úÖ SQLite database (no setup needed)
- ‚úÖ Simple in-memory caching
- ‚úÖ Local file storage
- ‚úÖ ROMA recursive planning
- ‚úÖ Parallel translation
- ‚úÖ Quality scoring

### Phase 2: Enhanced (Still Free!)
- ‚úÖ Hugging Face API fallback
- ‚úÖ Web interface (HTML/CSS/JS)
- ‚úÖ Discord bot (free tier)
- ‚úÖ File upload (PDF, DOCX)
- ‚úÖ Translation memory
- ‚úÖ Batch processing

### Phase 3: Free Deployment
- ‚úÖ Deploy to Render.com (free)
- ‚úÖ Deploy to Railway.app (free)
- ‚úÖ Deploy to Fly.io (free)
- ‚úÖ GitHub Pages for docs
- ‚úÖ Free SSL certificate

---

## üîß Configuration (Free Resources)

### Environment Variables (.env)

```bash
# ============================================
# LLM Configuration - FREE OPTIONS
# ============================================

# OPTION 1: Ollama (LOCAL - Completely FREE)
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
# Alternative models: mistral:7b, qwen2.5:7b

# OPTION 2: Hugging Face (FREE tier - 1000 req/day)
# LLM_PROVIDER=huggingface
# HUGGINGFACE_API_KEY=hf_xxxxx  # FREE, no credit card
# HUGGINGFACE_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# OPTION 3: Together AI (FREE $25 credits)
# LLM_PROVIDER=together
# TOGETHER_API_KEY=xxxxx
# TOGETHER_MODEL=meta-llama/Llama-3-8b-chat-hf

# OPTION 4: Groq (FREE tier - Very fast!)
# LLM_PROVIDER=groq
# GROQ_API_KEY=xxxxx  # FREE tier available
# GROQ_MODEL=llama-3.1-8b-instant

# Model parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2048  # Lower for free tiers
LLM_TIMEOUT=60

# ============================================
# Database - FREE (SQLite)
# ============================================
DATABASE_URL=sqlite:///data/translations.db
# No PostgreSQL needed!

# ============================================
# Cache - FREE (In-memory or simple file cache)
# ============================================
CACHE_ENABLED=true
CACHE_TYPE=memory  # memory or file
CACHE_DIR=data/cache
CACHE_TTL=86400

# ============================================
# File Storage - FREE (Local filesystem)
# ============================================
UPLOAD_DIR=data/uploads
MAX_UPLOAD_SIZE=10485760  # 10MB

# ============================================
# API Configuration
# ============================================
API_HOST=0.0.0.0
API_PORT=5000
API_WORKERS=2  # Lower for free hosting

# ============================================
# Feature Flags
# ============================================
ENABLE_TRANSLATION_MEMORY=true
ENABLE_QUALITY_CHECK=true
ENABLE_PARALLEL_TRANSLATION=true
ENABLE_FILE_UPLOAD=true

# ============================================
# Free Hosting Platform (Choose one)
# ============================================
# Render.com, Railway.app, or Fly.io
HOSTING_PLATFORM=render

# ============================================
# Application Settings
# ============================================
LOG_LEVEL=INFO
MAX_TEXT_LENGTH=10000  # Lower for free tier
MAX_TARGET_LANGUAGES=5  # Reasonable for free
MAX_CONCURRENT_TRANSLATIONS=3
```

---

## üì¶ Dependencies (100% Free)

### requirements.txt

```txt
# Core Framework (FREE)
agno>=0.1.0
# Note: Install ROMA by cloning the repo

# API Framework (FREE)
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
python-multipart>=0.0.6

# FREE LLM Integrations
ollama>=0.1.0           # Local models
huggingface-hub>=0.19.0 # HF Inference API
together>=0.2.0         # Together AI
groq>=0.4.0            # Groq API

# Database & Cache (FREE)
aiosqlite>=0.19.0      # Async SQLite (built-in Python)
# No PostgreSQL needed!

# NLP & Language (FREE)
langdetect>=1.0.9
lingua-language-detector>=2.0.0  # More accurate, free

# File Processing (FREE)
PyPDF2>=3.0.0
python-docx>=1.1.0
chardet>=5.2.0
markdown>=3.5.0

# CLI (FREE)
click>=8.1.0
rich>=13.7.0

# Testing (FREE)
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0

# Utilities (FREE)
python-dotenv>=1.0.0
pyyaml>=6.0
httpx>=0.25.0
```

---

## üéØ FREE LLM OPTIONS - Detailed Comparison

### Option 1: Ollama (LOCAL) ‚≠ê RECOMMENDED

**Pros:**
- ‚úÖ **Completely FREE forever** - no API costs
- ‚úÖ **100% Privacy** - data never leaves your machine
- ‚úÖ **No rate limits** - use as much as you want
- ‚úÖ **Works offline** - no internet needed
- ‚úÖ **Multiple models** - Llama, Mistral, Qwen, etc.
- ‚úÖ **Easy setup** - one command installation

**Cons:**
- ‚ùå Requires 8GB+ RAM (16GB recommended)
- ‚ùå Slower than cloud APIs (but still fast!)
- ‚ùå Needs decent CPU/GPU

**Setup:**
```bash
# Install Ollama (macOS/Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Or download from ollama.com for Windows

# Pull a model (choose one)
ollama pull llama3.1:8b      # Best balance (4.7GB)
ollama pull mistral:7b       # Faster, smaller (4.1GB)
ollama pull qwen2.5:7b       # Great multilingual (4.7GB)

# Start Ollama server
ollama serve

# Test
ollama run llama3.1:8b "Translate 'Hello' to Spanish"
```

**Model Recommendations:**
- **Best quality**: `llama3.1:8b` (Meta's latest)
- **Fastest**: `mistral:7b` (efficient)
- **Multilingual**: `qwen2.5:7b` (excellent for translation)

---

### Option 2: Hugging Face Inference API (FREE Tier)

**Pros:**
- ‚úÖ **FREE tier**: 1000 requests/day
- ‚úÖ **No credit card** required
- ‚úÖ **Cloud-based** - no local resources needed
- ‚úÖ **Many models** - Llama, Mistral, etc.
- ‚úÖ **Easy API** integration

**Cons:**
- ‚ùå Rate limited (1000/day on free tier)
- ‚ùå Slower inference than paid options
- ‚ùå Public internet required

**Setup:**
```bash
# Get FREE API key (no credit card)
# Visit: https://huggingface.co/settings/tokens

# Set in .env
HUGGINGFACE_API_KEY=hf_xxxxx
```

**Usage in Code:**
```python
from huggingface_hub import InferenceClient

client = InferenceClient(token=os.getenv("HUGGINGFACE_API_KEY"))
response = client.text_generation(
    model="meta-llama/Meta-Llama-3.1-8B-Instruct",
    prompt="Translate 'Hello' to Spanish",
    max_new_tokens=100
)
```

---

### Option 3: Together AI (FREE $25 Credits)

**Pros:**
- ‚úÖ **$25 free credits** on signup
- ‚úÖ **Fast inference**
- ‚úÖ **Multiple models**
- ‚úÖ **Good for testing**

**Cons:**
- ‚ùå Credits expire (but generous for dev/testing)
- ‚ùå Requires credit card for signup

**Setup:**
```bash
# Sign up: https://together.ai
# Get $25 free credits

# Set in .env
TOGETHER_API_KEY=xxxxx
```

---

### Option 4: Groq (FREE Tier) ‚ö° FASTEST

**Pros:**
- ‚úÖ **FREE tier** available
- ‚úÖ **EXTREMELY FAST** (fastest free option!)
- ‚úÖ **Great for demos**
- ‚úÖ **Llama 3.1 support**

**Cons:**
- ‚ùå Limited free requests
- ‚ùå Best for testing/demos, not production

**Setup:**
```bash
# Sign up: https://groq.com
# Get free API key

# Set in .env
GROQ_API_KEY=xxxxx
```

---

## üöÄ Implementation with FREE Resources

### LLM Service (FREE) - Unified Interface

**File: `src/services/llm_service.py`**

```python
import os
from typing import Optional
import ollama  # Local
from huggingface_hub import InferenceClient  # Free cloud
import groq  # Free tier

class FreeLLMService:
    """Unified interface for FREE LLM providers"""
    
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "ollama")
        self._init_provider()
    
    def _init_provider(self):
        """Initialize the chosen FREE provider"""
        
        if self.provider == "ollama":
            # LOCAL - Completely FREE
            self.client = ollama.Client(
                host=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            )
            self.model = os.getenv("OLLAMA_MODEL", "llama3.1:8b")
            print("‚úÖ Using Ollama (LOCAL - FREE)")
            
        elif self.provider == "huggingface":
            # FREE tier: 1000 requests/day
            self.client = InferenceClient(
                token=os.getenv("HUGGINGFACE_API_KEY")
            )
            self.model = os.getenv(
                "HUGGINGFACE_MODEL",
                "meta-llama/Meta-Llama-3.1-8B-Instruct"
            )
            print("‚úÖ Using Hugging Face (FREE tier)")
            
        elif self.provider == "groq":
            # FREE tier - Very fast!
            self.client = groq.Groq(
                api_key=os.getenv("GROQ_API_KEY")
            )
            self.model = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")
            print("‚úÖ Using Groq (FREE tier - Fast!)")
            
        else:
            raise ValueError(f"Unknown provider: {self.provider}")
    
    async def complete(self, prompt: str, max_tokens: int = 2048) -> str:
        """Generate completion - works with all FREE providers"""
        
        if self.provider == "ollama":
            response = self.client.generate(
                model=self.model,
                prompt=prompt,
                options={
                    "temperature": 0.3,
                    "num_predict": max_tokens
                }
            )
            return response['response']
            
        elif self.provider == "huggingface":
            response = self.client.text_generation(
                model=self.model,
                prompt=prompt,
                max_new_tokens=max_tokens,
                temperature=0.3
            )
            return response
            
        elif self.provider == "groq":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
```

---

### SQLite Database (FREE - Built-in)

**File: `src/services/database_service.py`**

```python
import aiosqlite
import os
from typing import List, Dict, Optional

class DatabaseService:
    """FREE SQLite database - no PostgreSQL needed!"""
    
    def __init__(self):
        self.db_path = os.getenv(
            "DATABASE_URL",
            "sqlite:///data/translations.db"
        ).replace("sqlite:///", "")
        
    async def initialize(self):
        """Create tables if they don't exist"""
        async with aiosqlite.connect(self.db_path) as db:
            # Translations table
            await db.execute("""
                CREATE TABLE IF NOT EXISTS translations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_text TEXT NOT NULL,
                    source_lang TEXT NOT NULL,
                    target_lang TEXT NOT NULL,
                    translation TEXT NOT NULL,
                    quality_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Translation memory table
            await db.execute("""
                CREATE TABLE IF NOT EXISTS translation_memory (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_hash TEXT UNIQUE NOT NULL,
                    source_text TEXT NOT NULL,
                    source_lang TEXT NOT NULL,
                    target_lang TEXT NOT NULL,
                    translation TEXT NOT NULL,
                    usage_count INTEGER DEFAULT 1,
                    last_used TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            await db.commit()
            print("‚úÖ SQLite database initialized (FREE!)")
    
    async def save_translation(
        self,
        source_text: str,
        source_lang: str,
        target_lang: str,
        translation: str,
        quality_score: float
    ):
        """Save translation for analytics"""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(
                """
                INSERT INTO translations 
                (source_text, source_lang, target_lang, translation, quality_score)
                VALUES (?, ?, ?, ?, ?)
                """,
                (source_text, source_lang, target_lang, translation, quality_score)
            )
            await db.commit()
    
    async def get_from_memory(
        self,
        source_text: str,
        source_lang: str,
        target_lang: str
    ) -> Optional[str]:
        """Get cached translation from memory"""
        import hashlib
        text_hash = hashlib.md5(source_text.encode()).hexdigest()
        
        async with aiosqlite.connect(self.db_path) as db:
            async with db.execute(
                """
                SELECT translation FROM translation_memory
                WHERE source_hash = ? AND source_lang = ? AND target_lang = ?
                """,
                (text_hash, source_lang, target_lang)
            ) as cursor:
                row = await cursor.fetchone()
                if row:
                    # Update usage count
                    await db.execute(
                        """
                        UPDATE translation_memory
                        SET usage_count = usage_count + 1,
                            last_used = CURRENT_TIMESTAMP
                        WHERE source_hash = ?
                        """,
                        (text_hash,)
                    )
                    await db.commit()
                    return row[0]
        return None
```

---

### Simple Cache (FREE - In-Memory)

**File: `src/services/cache_service.py`**

```python
from typing import Optional, Dict
import hashlib
import time

class SimpleCacheService:
    """FREE in-memory cache - no Redis needed!"""
    
    def __init__(self, ttl: int = 86400):
        self.cache: Dict[str, tuple] = {}  # key: (value, timestamp)
        self.ttl = ttl
        print("‚úÖ Using in-memory cache (FREE!)")
    
    def _make_key(
        self,
        text: str,
        source_lang: str,
        target_lang: str
    ) -> str:
        """Create cache key"""
        content = f"{text}:{source_lang}:{target_lang}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(
        self,
        text: str,
        source_lang: str,
        target_lang: str
    ) -> Optional[str]:
        """Get cached translation"""
        key = self._make_key(text, source_lang, target_lang)
        
        if key in self.cache:
            value, timestamp = self.cache[key]
            
            # Check if expired
            if time.time() - timestamp < self.ttl:
                print(f"‚úÖ Cache HIT for {source_lang}‚Üí{target_lang}")
                return value
            else:
                # Expired, remove
                del self.cache[key]
        
        print(f"‚ùå Cache MISS for {source_lang}‚Üí{target_lang}")
        return None
    
    def set(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        translation: str
    ):
        """Cache translation"""
        key = self._make_key(text, source_lang, target_lang)
        self.cache[key] = (translation, time.time())
    
    def clear_expired(self):
        """Periodically clear expired entries"""
        current_time = time.time()
        expired_keys = [
            k for k, (v, t) in self.cache.items()
            if current_time - t >= self.ttl
        ]
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            print(f"üßπ Cleared {len(expired_keys)} expired cache entries")
```

---

## üåê FREE Deployment Options

### Option 1: Render.com (RECOMMENDED) ‚≠ê

**render.yaml**
```yaml
services:
  - type: web
    name: roma-translation-bot
    env: python
    buildCommand: "pip install -r requirements.txt"
    startCommand: "uvicorn src.api.main:app --host 0.0.0.0 --port $PORT"
    plan: free  # FREE tier!
    envVars:
      - key: LLM_PROVIDER
        value: huggingface  # Use HF on free hosting
      - key: HUGGINGFACE_API_KEY
        sync: false  # Set in Render dashboard
      - key: PYTHON_VERSION
        value: 3.12.0
```

**Deployment Steps:**
1. Push code to GitHub
2. Sign up at render.com (FREE)
3. Create new "Web Service"
4. Connect GitHub repo
5. Render auto-detects Python
6. Deploy! (takes 5-10 min)

**FREE Tier Limits:**
- 750 hours/month (plenty!)
- Auto-sleep after inactivity
- Free SSL certificate

---

### Option 2: Railway.app

**railway.toml**
```toml
[build]
builder = "nixpacks"

[deploy]
startCommand = "uvicorn src.api.main:app --host 0.0.0.0 --port $PORT"
restartPolicyType = "on_failure"
```

**Deployment:**
1. Install Railway CLI: `npm i -g @railway/cli`
2. Login: `railway login`
3. Initialize: `railway init`
4. Deploy: `railway up`

**FREE Tier:**
- $5 free credits/month
- Enough for small-medium usage

---

### Option 3: Fly.io

**fly.toml**
```toml
app = "roma-translation-bot"

[build]
  builder = "paketobuildpacks/builder:base"

[[services]]
  internal_port = 5000
  protocol = "tcp"

  [[services.ports]]
    handlers = ["http"]
    port = 80

  [[services.ports]]
    handlers = ["tls", "http"]
    port = 443
```

**Deployment:**
```bash
# Install Fly CLI
curl -L https://fly.io/install.sh | sh

# Login
fly auth login

# Deploy
fly launch
fly deploy
```

**FREE Tier:**
- 3 shared-cpu VMs
- Perfect for our bot!

---

## üìä Cost Comparison (Monthly)

| Resource | Paid Option | Our FREE Option | Savings |
|----------|-------------|-----------------|---------|
| **LLM API** | $30-100+ | $0 (Ollama/HF free) | $30-100+ |
| **Database** | $15 (PostgreSQL) | $0 (SQLite) | $15 |
| **Cache** | $10 (Redis Cloud) | $0 (in-memory) | $10 |
| **Storage** | $5 (S3) | $0 (local/free host) | $5 |
| **Hosting** | $20 (AWS/GCP) | $0 (Render/Railway) | $20 |
| **SSL Certificate** | $10 | $0 (included free) | $10 |
| ****TOTAL**** | **$90-170+** | **$0** | **$90-170+** |

---

## üéØ FREE Setup Guide

### Quick Start (5 Minutes)

```bash
# 1. Clone ROMA
git clone https://github.com/sentient-agi/ROMA.git
cd ROMA

# 2. Setup ROMA (choose Docker or Native)
./setup.sh --docker

# 3. Create project directory
mkdir -p agents/translation-bot-free
cd agents/translation-bot-free

# 4. Clone this project
git init
# Add your code here

# 5. Setup FREE Ollama (LOCAL)
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1:8b

# 6. Create virtual environment
python3.12 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 7. Install dependencies
pip install -r requirements.txt

# 8. Configure environment
cp .env.example .env
# Edit .env: Set LLM_PROVIDER=ollama

# 9. Initialize database
python scripts/setup_db.py

# 10. Run the bot!
python -m src.cli translate "Hello world" --to spanish
```

---

## ‚úÖ Acceptance Criteria (FREE Edition)

### Must-Have (Phase 1)
- [ ] Works with FREE Ollama (local) - zero cost
- [ ] Alternative: FREE Hugging Face API integration
- [ ] Translates to single language in < 5 seconds (local)
- [ ] Translates to 3 languages in < 15 seconds (parallel)
- [ ] Supports 12+ languages
- [ ] CLI tool fully functional
- [ ] REST API with FastAPI
- [ ] SQLite database (no setup needed)
- [ ] Simple in-memory caching works
- [ ] Translation quality > 0.75 (acceptable for free models)
- [ ] All unit tests pass
- [ ] Runs on 8GB RAM machines
- [ ] Can deploy to Render.com free tier

### Nice-to-Have (Phase 2)
- [ ] File upload support (TXT, PDF, DOCX)
- [ ] Web UI (simple HTML/CSS/JS)
- [ ] Discord bot integration
- [ ] Translation memory improves speed
- [ ] Batch processing
- [ ] Deploy to multiple free platforms

---

## üéØ Performance Expectations (FREE Models)

### Realistic Benchmarks

| Metric | Ollama (Local) | HuggingFace Free | Target |
|--------|----------------|------------------|--------|
| Simple translation | 3-5s | 5-10s | < 10s |
| Multi-language (3) | 10-15s | 20-30s | < 30s |
| Quality score | 0.75-0.85 | 0.70-0.80 | > 0.70 |
| Cache hit improvement | 95% faster | 95% faster | > 90% |
| Concurrent requests | 5-10 | 2-5 | > 3 |

**Note:** Free models are slower but still highly usable!

---

## üîí Security (FREE but Secure)

Even with free resources, maintain security:

1. **Local Models (Ollama)**
   - ‚úÖ Data never leaves your machine
   - ‚úÖ Complete privacy
   - ‚úÖ No API keys to leak

2. **Free APIs (HuggingFace)**
   - ‚úÖ Store API keys in .env (never commit)
   - ‚úÖ Use environment variables
   - ‚úÖ Rate limit to stay in free tier

3. **Input Validation**
   - ‚úÖ Validate all user inputs
   - ‚úÖ Sanitize file uploads
   - ‚úÖ Limit text length

4. **SQLite Security**
   - ‚úÖ Use parameterized queries
   - ‚úÖ No SQL injection possible
   - ‚úÖ File permissions set correctly

---

## üöÄ Step-by-Step Implementation (FREE)

### Day 1: Environment Setup (FREE)

```bash
# Install FREE Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Download FREE model (choose one)
ollama pull llama3.1:8b      # Recommended (4.7GB)
# OR
ollama pull mistral:7b       # Faster (4.1GB)
# OR
ollama pull qwen2.5:7b       # Best multilingual (4.7GB)

# Test Ollama
ollama run llama3.1:8b "Translate 'Hello' to Spanish"

# Clone ROMA
git clone https://github.com/sentient-agi/ROMA.git
cd ROMA
./setup.sh --docker

# Create project
mkdir -p agents/translation-bot-free
cd agents/translation-bot-free

# Python environment
python3.12 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements-free.txt
```

**requirements-free.txt**
```txt
# Minimal FREE dependencies
agno>=0.1.0
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
aiosqlite>=0.19.0
ollama>=0.1.0
huggingface-hub>=0.19.0
langdetect>=1.0.9
PyPDF2>=3.0.0
python-docx>=1.1.0
click>=8.1.0
rich>=13.7.0
pytest>=7.4.0
python-dotenv>=1.0.0
pyyaml>=6.0
```

---

### Day 2: Core Implementation

**1. FREE LLM Service**
```python
# src/services/llm_service.py
import ollama
from huggingface_hub import InferenceClient
import os

class FreeLLMService:
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "ollama")
        
        if self.provider == "ollama":
            self.client = ollama.Client()
            self.model = os.getenv("OLLAMA_MODEL", "llama3.1:8b")
        elif self.provider == "huggingface":
            self.client = InferenceClient(
                token=os.getenv("HUGGINGFACE_API_KEY")
            )
            self.model = "meta-llama/Meta-Llama-3.1-8B-Instruct"
    
    async def translate(
        self, 
        text: str, 
        source_lang: str, 
        target_lang: str
    ) -> str:
        prompt = f"""Translate this text from {source_lang} to {target_lang}.
Provide ONLY the translation, no explanations.

Text: {text}

Translation:"""
        
        if self.provider == "ollama":
            response = self.client.generate(
                model=self.model,
                prompt=prompt,
                options={"temperature": 0.3}
            )
            return response['response'].strip()
        
        elif self.provider == "huggingface":
            response = self.client.text_generation(
                model=self.model,
                prompt=prompt,
                max_new_tokens=500,
                temperature=0.3
            )
            return response.strip()
```

**2. Translation Agent with ROMA**
```python
# src/core/translation_agent.py
from src.services.llm_service import FreeLLMService
from src.services.cache_service import SimpleCacheService
from src.services.database_service import DatabaseService
import asyncio
from typing import List, Dict
import time

class TranslationBot:
    def __init__(self):
        self.llm = FreeLLMService()
        self.cache = SimpleCacheService()
        self.db = DatabaseService()
        
    async def translate(
        self,
        text: str,
        target_languages: List[str],
        source_language: str = None,
        preserve_formatting: bool = True
    ) -> Dict:
        start_time = time.time()
        
        # Detect source language if not provided
        if not source_language:
            source_language = await self._detect_language(text)
        
        # Check if complex (multiple languages) - ROMA Atomizer logic
        if len(target_languages) == 1:
            # Atomic task - execute directly
            translation = await self._single_translate(
                text, source_language, target_languages[0]
            )
            translations = {target_languages[0]: translation}
        else:
            # Complex task - ROMA Planner breaks into parallel subtasks
            translations = await self._parallel_translate(
                text, source_language, target_languages
            )
        
        # Calculate quality scores
        quality_scores = await self._calculate_quality(
            text, translations, source_language
        )
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return {
            "request_id": self._generate_id(),
            "source_language": source_language,
            "translations": translations,
            "quality_scores": quality_scores,
            "processing_time_ms": processing_time,
            "cached": False,
            "metadata": {
                "model": self.llm.model,
                "provider": self.llm.provider,
                "parallel_execution": len(target_languages) > 1
            }
        }
    
    async def _single_translate(
        self, 
        text: str, 
        source: str, 
        target: str
    ) -> str:
        # Check cache first
        cached = self.cache.get(text, source, target)
        if cached:
            return cached
        
        # Check translation memory (database)
        memory = await self.db.get_from_memory(text, source, target)
        if memory:
            self.cache.set(text, source, target, memory)
            return memory
        
        # Translate with FREE LLM
        translation = await self.llm.translate(text, source, target)
        
        # Cache and save
        self.cache.set(text, source, target, translation)
        await self.db.save_to_memory(text, source, target, translation)
        
        return translation
    
    async def _parallel_translate(
        self,
        text: str,
        source: str,
        targets: List[str]
    ) -> Dict[str, str]:
        # ROMA Executor - parallel execution of subtasks
        tasks = [
            self._single_translate(text, source, target)
            for target in targets
        ]
        
        results = await asyncio.gather(*tasks)
        
        # ROMA Aggregator - combine results
        return {
            target: result
            for target, result in zip(targets, results)
        }
    
    async def _detect_language(self, text: str) -> str:
        import langdetect
        try:
            return langdetect.detect(text)
        except:
            return "en"  # Default to English
    
    async def _calculate_quality(
        self, 
        source_text: str, 
        translations: Dict[str, str],
        source_lang: str
    ) -> Dict[str, float]:
        # Simple quality check based on length ratio
        quality_scores = {}
        source_len = len(source_text.split())
        
        for lang, translation in translations.items():
            trans_len = len(translation.split())
            ratio = trans_len / source_len if source_len > 0 else 0
            
            # Good translations typically have 0.8-1.5x word count
            if 0.5 <= ratio <= 2.0:
                quality_scores[lang] = min(0.9, 0.6 + (1.0 - abs(1.0 - ratio)))
            else:
                quality_scores[lang] = 0.5
        
        return quality_scores
    
    def _generate_id(self) -> str:
        import uuid
        return str(uuid.uuid4())
```

---

### Day 3: CLI Interface (FREE)

```python
# src/cli/commands.py
import asyncio
import click
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from src.core.translation_agent import TranslationBot

console = Console()

@click.group()
def cli():
    """FREE ROMA Translation Bot - Zero Cost, Powered by Ollama"""
    pass

@cli.command()
@click.argument('text')
@click.option('--to', '-t', multiple=True, required=True, help='Target languages')
@click.option('--from', '-f', 'source', default=None, help='Source language')
def translate(text, to, source):
    """Translate text using FREE local models"""
    
    async def run():
        bot = TranslationBot()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task(
                f"[cyan]Translating to {len(to)} language(s)...",
                total=None
            )
            
            result = await bot.translate(
                text=text,
                target_languages=list(to),
                source_language=source
            )
        
        # Display results
        console.print(f"\n[green]‚úì[/green] Translation complete!", style="bold")
        console.print(f"[dim]Source: {result['source_language']}")
        console.print(f"[dim]Provider: {result['metadata']['provider']} ({result['metadata']['model']})")
        console.print(f"[dim]Time: {result['processing_time_ms']}ms\n")
        
        table = Table(title="Translations")
        table.add_column("Language", style="cyan", width=15)
        table.add_column("Translation", style="green")
        table.add_column("Quality", style="yellow", width=10)
        
        for lang in to:
            quality = result['quality_scores'].get(lang, 0)
            quality_str = f"{quality:.2f}"
            table.add_row(
                lang.capitalize(),
                result['translations'][lang],
                quality_str
            )
        
        console.print(table)
    
    asyncio.run(run())

@cli.command()
@click.argument('file_path', type=click.Path(exists=True))
@click.option('--to', '-t', multiple=True, required=True)
@click.option('--from', '-f', 'source', default=None)
def translate_file(file_path, to, source):
    """Translate entire file (FREE)"""
    
    async def run():
        # Read file
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        console.print(f"[cyan]Reading file: {file_path}")
        console.print(f"[dim]Size: {len(text)} characters\n")
        
        bot = TranslationBot()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
        ) as progress:
            task = progress.add_task(
                "[cyan]Translating file...",
                total=None
            )
            
            result = await bot.translate(
                text=text,
                target_languages=list(to),
                source_language=source
            )
        
        # Save translations
        import os
        base_name = os.path.splitext(file_path)[0]
        
        for lang, translation in result['translations'].items():
            output_file = f"{base_name}.{lang}.txt"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(translation)
            console.print(f"[green]‚úì[/green] Saved: {output_file}")
        
        console.print(f"\n[green]Done![/green] ({result['processing_time_ms']}ms)")
    
    asyncio.run(run())

@cli.command()
def languages():
    """List supported languages (FREE)"""
    
    languages = [
        ("en", "English"),
        ("es", "Spanish"),
        ("fr", "French"),
        ("de", "German"),
        ("it", "Italian"),
        ("pt", "Portuguese"),
        ("ja", "Japanese"),
        ("zh", "Chinese"),
        ("ko", "Korean"),
        ("ru", "Russian"),
        ("ar", "Arabic"),
        ("hi", "Hindi"),
    ]
    
    table = Table(title="Supported Languages (FREE Translation)")
    table.add_column("Code", style="cyan", width=10)
    table.add_column("Language", style="green")
    
    for code, name in languages:
        table.add_row(code, name)
    
    console.print(table)

@cli.command()
@click.argument('text')
def detect(text):
    """Detect language (FREE)"""
    
    async def run():
        bot = TranslationBot()
        lang = await bot._detect_language(text)
        console.print(f"\n[green]Detected language:[/green] [cyan]{lang}[/cyan]")
    
    asyncio.run(run())

@cli.command()
def info():
    """Show FREE resources info"""
    
    import ollama
    
    console.print("\n[bold cyan]ROMA Translation Bot[/bold cyan] - FREE Edition")
    console.print("[dim]Powered by 100% free and open-source resources\n")
    
    # Check Ollama
    try:
        models = ollama.list()
        console.print("[green]‚úì[/green] Ollama is running")
        console.print(f"[dim]  Available models: {len(models['models'])}")
        for model in models['models'][:3]:
            console.print(f"[dim]  - {model['name']}")
    except:
        console.print("[red]‚úó[/red] Ollama not found")
        console.print("[dim]  Install: curl -fsSL https://ollama.com/install.sh | sh")
    
    # Check database
    import os
    if os.path.exists("data/translations.db"):
        console.print("[green]‚úì[/green] SQLite database ready")
    else:
        console.print("[yellow]![/yellow] Database not initialized")
        console.print("[dim]  Run: python scripts/setup_db.py")
    
    console.print()

if __name__ == '__main__':
    cli()
```

---

### Day 4: REST API (FREE)

```python
# src/api/main.py
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
from src.core.translation_agent import TranslationBot
import time

app = FastAPI(
    title="ROMA Translation Bot (FREE)",
    description="Zero-cost translation API powered by free open-source models",
    version="1.0.0"
)

# CORS (allow all for free tier)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize bot
bot = TranslationBot()

# Request/Response Models
class TranslationRequest(BaseModel):
    text: str = Field(..., max_length=10000, description="Text to translate")
    target_languages: List[str] = Field(
        ..., 
        min_items=1, 
        max_items=5,
        description="Target languages (max 5 for free tier)"
    )
    source_language: Optional[str] = Field(None, description="Source language")
    preserve_formatting: bool = Field(True, description="Preserve formatting")

class TranslationResponse(BaseModel):
    request_id: str
    source_language: str
    translations: Dict[str, str]
    quality_scores: Dict[str, float]
    processing_time_ms: int
    cached: bool
    metadata: Dict

# Simple rate limiting for free tier
request_counts = {}

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    """Simple rate limiting - 20 requests per minute (FREE tier)"""
    
    client_ip = request.client.host
    current_minute = int(time.time() / 60)
    key = f"{client_ip}:{current_minute}"
    
    if key in request_counts:
        request_counts[key] += 1
        if request_counts[key] > 20:
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Rate limit exceeded",
                    "message": "Free tier: 20 requests per minute",
                    "retry_after": 60
                }
            )
    else:
        request_counts[key] = 1
    
    # Cleanup old entries
    old_keys = [k for k in request_counts.keys() if int(k.split(':')[1]) < current_minute - 5]
    for k in old_keys:
        del request_counts[k]
    
    response = await call_next(request)
    return response

@app.post("/api/v1/translate", response_model=TranslationResponse)
async def translate_text(request: TranslationRequest):
    """
    Translate text to multiple languages (FREE)
    
    Uses 100% free resources:
    - Ollama (local models) or
    - Hugging Face free tier
    """
    
    try:
        result = await bot.translate(
            text=request.text,
            target_languages=request.target_languages,
            source_language=request.source_language,
            preserve_formatting=request.preserve_formatting
        )
        
        return TranslationResponse(**result)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/languages")
async def get_languages():
    """List supported languages (FREE)"""
    return {
        "languages": [
            {"code": "en", "name": "English"},
            {"code": "es", "name": "Spanish"},
            {"code": "fr", "name": "French"},
            {"code": "de", "name": "German"},
            {"code": "it", "name": "Italian"},
            {"code": "pt", "name": "Portuguese"},
            {"code": "ja", "name": "Japanese"},
            {"code": "zh", "name": "Chinese"},
            {"code": "ko", "name": "Korean"},
            {"code": "ru", "name": "Russian"},
            {"code": "ar", "name": "Arabic"},
            {"code": "hi", "name": "Hindi"},
        ],
        "total": 12,
        "cost": "$0 - Completely FREE!"
    }

@app.post("/api/v1/detect")
async def detect_language(text: str):
    """Detect language (FREE)"""
    lang = await bot._detect_language(text)
    return {"language": lang, "text": text}

@app.get("/api/v1/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "provider": bot.llm.provider,
        "model": bot.llm.model,
        "cost": "$0 - FREE forever!",
        "limits": {
            "max_text_length": 10000,
            "max_languages": 5,
            "rate_limit": "20 req/min"
        }
    }

@app.get("/")
async def root():
    """API info"""
    return {
        "name": "ROMA Translation Bot (FREE Edition)",
        "version": "1.0.0",
        "description": "Zero-cost translation API powered by open-source models",
        "cost": "$0 - Completely FREE!",
        "endpoints": {
            "translate": "POST /api/v1/translate",
            "languages": "GET /api/v1/languages",
            "detect": "POST /api/v1/detect",
            "health": "GET /api/v1/health"
        },
        "docs": "/docs",
        "resources": {
            "llm": "Ollama (local) or Hugging Face (free tier)",
            "database": "SQLite (built-in)",
            "cache": "In-memory (free)",
            "hosting": "Render.com / Railway.app (free tiers)"
        }
    }

# Run with: uvicorn src.api.main:app --reload
```

---

### Day 5: Deploy to FREE Hosting

**1. Render.com Deployment**

Create `render.yaml`:
```yaml
services:
  - type: web
    name: roma-translation-free
    env: python
    region: oregon
    plan: free
    buildCommand: "pip install -r requirements-free.txt && python scripts/setup_db.py"
    startCommand: "uvicorn src.api.main:app --host 0.0.0.0 --port $PORT"
    envVars:
      - key: LLM_PROVIDER
        value: huggingface
      - key: HUGGINGFACE_API_KEY
        sync: false
      - key: PYTHON_VERSION
        value: 3.12.0
    healthCheckPath: /api/v1/health
```

**Deployment Steps:**
1. Push code to GitHub
2. Go to render.com ‚Üí Sign up (FREE)
3. Click "New" ‚Üí "Web Service"
4. Connect GitHub repo
5. Render detects render.yaml
6. Click "Create Web Service"
7. Done! ‚úÖ

**Free Tier Details:**
- 750 hours/month
- Auto-sleeps after 15 min inactivity
- Wakes up automatically on request
- FREE SSL certificate included

---

## üéì Usage Examples (FREE)

### CLI Usage

```bash
# Simple translation (FREE)
python -m src.cli translate "Hello world" --to spanish
# Output: "Hola mundo" (using FREE Ollama)

# Multiple languages (FREE)
python -m src.cli translate "Good morning" --to spanish --to french --to german

# Translate file (FREE)
python -m src.cli translate-file document.txt --to spanish

# Check info
python -m src.cli info

# List languages
python -m src.cli languages
```

### API Usage (FREE)

```bash
# Translate via FREE API
curl -X POST http://localhost:5000/api/v1/translate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello world",
    "target_languages": ["spanish", "french"]
  }'

# Response (from FREE model!)
{
  "request_id": "abc-123",
  "source_language": "en",
  "translations": {
    "spanish": "Hola mundo",
    "french": "Bonjour le monde"
  },
  "quality_scores": {
    "spanish": 0.85,
    "french": 0.82
  },
  "processing_time_ms": 3421,
  "metadata": {
    "provider": "ollama",
    "model": "llama3.1:8b",
    "cost": "$0 - FREE!"
  }
}
```

### Python SDK (FREE)

```python
from src.core.translation_agent import TranslationBot
import asyncio

async def main():
    # Initialize FREE bot
    bot = TranslationBot()  # Uses FREE Ollama by default
    
    # Translate (FREE)
    result = await bot.translate(
        text="Hello, how are you?",
        target_languages=["spanish", "french", "german"]
    )
    
    print(f"Translations: {result['translations']}")
    print(f"Cost: $0 - Completely FREE!")

asyncio.run(main())
```

---

## üìö Additional FREE Resources

### Setup Scripts

**scripts/setup_ollama.sh**
```bash
#!/bin/bash
# Setup FREE Ollama models

echo "üöÄ Setting up FREE Ollama models..."

# Install Ollama (if not installed)
if ! command -v ollama &> /dev/null; then
    echo "üì• Installing Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
fi

# Pull FREE models
echo "üì¶ Downloading FREE models (this may take a few minutes)..."

# Option 1: Llama 3.1 8B (Recommended)
ollama pull llama3.1:8b
echo "‚úÖ Llama 3.1 8B installed (4.7GB)"

# Option 2: Mistral 7B (Alternative)
# ollama pull mistral:7b
# echo "‚úÖ Mistral 7B installed (4.1GB)"

# Start Ollama server
ollama serve &

echo "üéâ FREE Ollama setup complete!"
echo "üí∞ Cost: $0 - Completely FREE forever!"
```

**scripts/setup_db.py**
```python
#!/usr/bin/env python3
"""Initialize FREE SQLite database"""

import asyncio
import os
from src.services.database_service import DatabaseService

async def main():
    print("üóÑÔ∏è  Setting up FREE SQLite database...")
    
    # Create data directory
    os.makedirs("data", exist_ok=True)
    
    # Initialize database
    db = DatabaseService()
    await db.initialize()
    
    print("‚úÖ Database initialized!")
    print("üí∞ Cost: $0 - FREE forever!")
    print(f"üìÅ Location: {db.db_path}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## üéâ Final Notes

### What Makes This FREE?

1. **Ollama (LOCAL)** - Run LLMs on your machine
   - No API costs ever
   - Complete privacy
   - Works offline

2. **SQLite** - Built into Python
   - No PostgreSQL hosting needed
   - Zero setup
   - Perfect for most use cases

3. **In-Memory Cache** - No Redis hosting needed
   - Simple Python dictionary
   - Fast enough for most cases

4. **Free Hosting** - Render.com/Railway/Fly.io
   - Generous free tiers
   - Auto-deploy from GitHub
   - Free SSL certificates

5. **Open-Source Everything**
   - ROMA framework (MIT)
   - Python libraries (all free)
   - No vendor lock-in

### Total Monthly Cost

**$0.00** üéâ

### Perfect For

- ‚úÖ Independent developers
- ‚úÖ Students learning AI
- ‚úÖ Small projects
- ‚úÖ Privacy-conscious users
- ‚úÖ Offline usage
- ‚úÖ Budget-conscious teams

### Upgrade Path

When you're ready to scale:
1. Keep using free Ollama locally
2. Or upgrade to paid APIs (Claude, GPT) for better quality
3. Move to PostgreSQL for better concurrency
4. Add Redis for distributed caching
5. Deploy to paid hosting for more resources

But you can start with **$0** and stay at **$0** indefinitely! üöÄ

---

**Built with ‚ù§Ô∏è using 100% FREE resources**

*No credit card required. No hidden costs. Just pure open-source goodness.*